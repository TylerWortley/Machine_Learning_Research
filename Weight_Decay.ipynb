{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feed Forward Neural Network\n",
    "- 1 input, 1 output, with 1 hudden layer and 2 neurons with some weights, 100 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def activation_function(x, func): # could add ReLU, Leaky ReLU, or Softmax\n",
    "    if func == 'sigmoid':\n",
    "        return(1/(1 + np.exp(-x)))\n",
    "    elif func == 'tanh':\n",
    "        return np.tanh(x)\n",
    "\n",
    "\n",
    "class FFNN_Generator(object):\n",
    "\n",
    "    def __init__(self, node_layers:list[int]=[]):\n",
    "        self.node_layers = node_layers\n",
    "        self.weight_matrix = []\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        if len(self.node_layers) > 1:\n",
    "\n",
    "            for i in range(len(self.node_layers)-1):\n",
    "                # rows of the weight matrix are the from # of inputs\n",
    "                rows = self.node_layers[i]\n",
    "                # columns of the weight matrix are the neurons in the layer\n",
    "                columns = self.node_layers[i+1]\n",
    "                weights = np.random.rand(rows, columns)\n",
    "                self.weight_matrix.append(weights)\n",
    "    \n",
    "\n",
    "    def feedforward(self, input):\n",
    "        I = input # I is input to the next layer\n",
    "        out = None\n",
    "        print(self.weight_matrix)\n",
    "        for i, weight in enumerate(self.weight_matrix):\n",
    "            I_W = np.dot(I, weight)\n",
    "\n",
    "            if i == (len(self.weight_matrix) - 1):\n",
    "                out = activation_function(I_W, 'tanh') # output layer\n",
    "            else:\n",
    "                I = activation_function(I_W, 'tanh') # updating inputs for hidden layers\n",
    "        return out # output matrix shape (1x100)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# instantiate this neural network\n",
    "Data_Generator = FFNN_Generator([1, 2, 1])\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# input 1, 1 hidden layer (2 neurons therefore 1*2 = 2 weights), 1 output\n",
    "# hidden layer (2 weights * 100 inputs) -> shape is (2x100)\n",
    "# output matrix shape (1x100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data From First Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.13415972, 0.77896523]]), array([[0.65077867],\n",
      "       [0.80610604]])]\n",
      "Generated Data: [[0.06495402]\n",
      " [0.10716335]\n",
      " [0.18573657]\n",
      " [0.35556314]\n",
      " [0.08957697]\n",
      " [0.1048441 ]\n",
      " [0.15406612]\n",
      " [0.14264992]\n",
      " [0.20959596]\n",
      " [0.03532803]\n",
      " [0.34890129]\n",
      " [0.5291491 ]\n",
      " [0.39695794]\n",
      " [0.48761195]\n",
      " [0.01950436]\n",
      " [0.44595905]\n",
      " [0.29469663]\n",
      " [0.44101116]\n",
      " [0.53569673]\n",
      " [0.03939715]\n",
      " [0.43007495]\n",
      " [0.33242306]\n",
      " [0.19594459]\n",
      " [0.2131714 ]\n",
      " [0.43549025]\n",
      " [0.00644574]\n",
      " [0.26442265]\n",
      " [0.36950802]\n",
      " [0.42749324]\n",
      " [0.2573125 ]\n",
      " [0.01010271]\n",
      " [0.36956847]\n",
      " [0.22598938]\n",
      " [0.39451608]\n",
      " [0.18567035]\n",
      " [0.33246057]\n",
      " [0.41040141]\n",
      " [0.05116256]\n",
      " [0.19826881]\n",
      " [0.09786225]\n",
      " [0.54335612]\n",
      " [0.17428399]\n",
      " [0.51959849]\n",
      " [0.18735889]\n",
      " [0.28903671]\n",
      " [0.47504317]\n",
      " [0.32190977]\n",
      " [0.54057702]\n",
      " [0.49147542]\n",
      " [0.00586968]\n",
      " [0.34725016]\n",
      " [0.03318431]\n",
      " [0.06260217]\n",
      " [0.10326137]\n",
      " [0.28246393]\n",
      " [0.14541083]\n",
      " [0.33389576]\n",
      " [0.29585572]\n",
      " [0.51748054]\n",
      " [0.33614345]\n",
      " [0.28930333]\n",
      " [0.53393815]\n",
      " [0.43660106]\n",
      " [0.04196771]\n",
      " [0.37815845]\n",
      " [0.47171516]\n",
      " [0.46899449]\n",
      " [0.21496745]\n",
      " [0.35124113]\n",
      " [0.42539831]\n",
      " [0.19565594]\n",
      " [0.28935843]\n",
      " [0.34209606]\n",
      " [0.22435042]\n",
      " [0.19408174]\n",
      " [0.30703023]\n",
      " [0.11514467]\n",
      " [0.26955149]\n",
      " [0.27769001]\n",
      " [0.53519566]\n",
      " [0.47101359]\n",
      " [0.49908326]\n",
      " [0.05383145]\n",
      " [0.17528564]\n",
      " [0.5319075 ]\n",
      " [0.50656702]\n",
      " [0.09571376]\n",
      " [0.50719582]\n",
      " [0.2850498 ]\n",
      " [0.07662109]\n",
      " [0.14887516]\n",
      " [0.37112461]\n",
      " [0.21630626]\n",
      " [0.39813684]\n",
      " [0.35260462]\n",
      " [0.5106531 ]\n",
      " [0.15749416]\n",
      " [0.52270082]\n",
      " [0.20872396]\n",
      " [0.14485945]]\n",
      "Feed Forward Neural Network 1 Weights: [array([[0.13415972, 0.77896523]]), array([[0.65077867],\n",
      "       [0.80610604]])]\n"
     ]
    }
   ],
   "source": [
    "input = np.random.rand(100, 1)\n",
    "data = Data_Generator.feedforward(input)\n",
    "\n",
    "print(\"Generated Data:\", data)\n",
    "print(\"Feed Forward Neural Network 1 Weights:\", Data_Generator.weight_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Second Neural Network\n",
    "- use data created by first neural network to train this second RNN \n",
    "- use adam optimizer to investigate weight decay\n",
    "- 2 scenarios: firstly, leave at default, no regularization, second, give some value that is supposed to help training (maybe look at adam and weight decay parameter documentaiton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Predictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FFNN_Predictor, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.ho = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.ho(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# instantiate second network\n",
    "predictor = FFNN_Predictor(input_size = data.shape[1], hidden_size = 2, output_size = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Weight Decay with Two Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(FFNN_Predictor.parameters, lr=learning_rate) # FFNN_parameters provides the parameters (weight and biases) to be updated as training progresses\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
